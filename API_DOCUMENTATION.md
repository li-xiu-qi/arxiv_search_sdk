# ArXiv Search SDK API æ–‡æ¡£

## æ¦‚è¿°

ArXiv Search SDK æ˜¯ä¸€ä¸ªåŠŸèƒ½å¼ºå¤§çš„ Python åº“ï¼Œä¸“é—¨ç”¨äºæœç´¢å’Œè·å– arXiv è®ºæ–‡ã€‚å®ƒæä¾›äº†ç®€å•æ˜“ç”¨çš„å¼‚æ­¥APIï¼Œæ”¯æŒå¤šç§æœç´¢æ–¹å¼ï¼ŒåŒ…æ‹¬æŒ‰æ ‡é¢˜ã€ä½œè€…ã€æ‘˜è¦ã€åˆ†ç±»ç­‰å­—æ®µè¿›è¡Œæœç´¢ã€‚

## æ ¸å¿ƒç‰¹æ€§

- ğŸ” **å¤šå­—æ®µæœç´¢**: æ”¯æŒæŒ‰æ ‡é¢˜ã€ä½œè€…ã€æ‘˜è¦ã€åˆ†ç±»ç­‰å­—æ®µæœç´¢
- ğŸ¯ **æ™ºèƒ½æŸ¥è¯¢**: è‡ªåŠ¨ä¼˜åŒ–æœç´¢æŸ¥è¯¢ä»¥æé«˜ç»“æœç›¸å…³æ€§
- ğŸ“š **åˆ†ç±»æµè§ˆ**: æ”¯æŒæŒ‰å­¦ç§‘åˆ†ç±»æµè§ˆè®ºæ–‡
- ğŸŒ **å¼‚æ­¥æ”¯æŒ**: ä½¿ç”¨ asyncio æä¾›é«˜æ€§èƒ½çš„å¼‚æ­¥æœç´¢
- ğŸ“Š **æ•°æ®æ¨¡å‹**: ä½¿ç”¨ Pydantic æä¾›ç±»å‹å®‰å…¨çš„æ•°æ®æ¨¡å‹

## å®‰è£…

```bash
pip install arxiv-search-sdk
```

## å¿«é€Ÿå¼€å§‹

```python
import asyncio
from arxiv_search import ArxivClient

async def main():
    async with ArxivClient() as client:
        # æœç´¢æœºå™¨å­¦ä¹ ç›¸å…³è®ºæ–‡
        papers = await client.search_ml_papers(max_results=10)
        
        for paper in papers:
            print(f"Title: {paper.title}")
            print(f"Authors: {', '.join(paper.authors)}")
            print(f"PDF: {paper.pdf_url}")
            print("-" * 50)

asyncio.run(main())
```

## æ ¸å¿ƒç±»å’Œæ–¹æ³•

### ArxivClient ç±»

`ArxivClient` æ˜¯ä¸»è¦çš„å®¢æˆ·ç«¯ç±»ï¼Œæä¾›æ‰€æœ‰æœç´¢åŠŸèƒ½ã€‚

#### åˆå§‹åŒ–

```python
from arxiv_search import ArxivClient

# åˆ›å»ºå®¢æˆ·ç«¯å®ä¾‹
client = ArxivClient(timeout=30)  # å¯é€‰ï¼šè®¾ç½®è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰

# æ¨èä½¿ç”¨å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨
async with ArxivClient() as client:
    # åœ¨è¿™é‡Œè¿›è¡Œæœç´¢æ“ä½œ
    pass
```

#### åŸºæœ¬æœç´¢æ–¹æ³•

##### `search_papers(query: SearchQuery) -> SearchResult`

æ‰§è¡Œæœç´¢æŸ¥è¯¢ï¼Œè¿”å›å®Œæ•´çš„æœç´¢ç»“æœå¯¹è±¡ã€‚

```python
from arxiv_search import ArxivClient, SearchQuery, SearchFieldQuery

async with ArxivClient() as client:
    query = SearchQuery(
        original_query="machine learning",
        field_queries=[
            SearchFieldQuery(field="ti", terms=["machine learning"]),
            SearchFieldQuery(field="abs", terms=["neural network"])
        ],
        max_results=10,
        sort_by="relevance"
    )
    
    result = await client.search_papers(query)
    print(f"æ‰¾åˆ° {result.total_found} ç¯‡è®ºæ–‡")
    for paper in result.papers:
        print(paper.title)
```

##### `search_by_title(title_terms: List[str], max_results: int = 10) -> List[Paper]`

æŒ‰æ ‡é¢˜æœç´¢è®ºæ–‡ã€‚

```python
async with ArxivClient() as client:
    papers = await client.search_by_title(
        title_terms=["transformer", "attention"],
        max_results=5
    )
    
    for paper in papers:
        print(f"Title: {paper.title}")
        print(f"ArXiv ID: {paper.arxiv_id}")
```

##### `search_by_author(author_names: List[str], max_results: int = 10) -> List[Paper]`

æŒ‰ä½œè€…æœç´¢è®ºæ–‡ã€‚

```python
async with ArxivClient() as client:
    papers = await client.search_by_author(
        author_names=["Geoffrey Hinton"],
        max_results=5
    )
    
    for paper in papers:
        print(f"Title: {paper.title}")
        print(f"Authors: {', '.join(paper.authors)}")
```

##### `search_by_abstract(abstract_terms: List[str], max_results: int = 10) -> List[Paper]`

æŒ‰æ‘˜è¦å…³é”®è¯æœç´¢è®ºæ–‡ã€‚

```python
async with ArxivClient() as client:
    papers = await client.search_by_abstract(
        abstract_terms=["deep learning", "neural network"],
        max_results=5
    )
    
    for paper in papers:
        print(f"Title: {paper.title}")
        print(f"Abstract: {paper.abstract[:100]}...")
```

##### `search_by_category(categories: List[str], max_results: int = 10) -> List[Paper]`

æŒ‰å­¦ç§‘åˆ†ç±»æœç´¢è®ºæ–‡ã€‚

```python
async with ArxivClient() as client:
    papers = await client.search_by_category(
        categories=["cs.AI", "cs.LG"],
        max_results=10
    )
    
    for paper in papers:
        print(f"Title: {paper.title}")
        print(f"Categories: {', '.join(paper.categories)}")
```

#### é«˜çº§æœç´¢æ–¹æ³•

##### `advanced_search(**kwargs) -> List[Paper]`

æ‰§è¡Œé«˜çº§ç»„åˆæœç´¢ã€‚

```python
async with ArxivClient() as client:
    papers = await client.advanced_search(
        title_terms=["transformer"],
        abstract_terms=["attention"],
        categories=["cs.AI", "cs.LG"],
        date_start="202301010000",  # 2023å¹´1æœˆ1æ—¥
        date_end="202312312359",    # 2023å¹´12æœˆ31æ—¥
        max_results=20,
        sort_by="submittedDate"
    )
    
    for paper in papers:
        print(f"Title: {paper.title}")
        print(f"Published: {paper.published}")
```

**å‚æ•°è¯´æ˜:**

- `title_terms`: æ ‡é¢˜å…³é”®è¯åˆ—è¡¨
- `author_names`: ä½œè€…å§“ååˆ—è¡¨
- `abstract_terms`: æ‘˜è¦å…³é”®è¯åˆ—è¡¨
- `categories`: å­¦ç§‘åˆ†ç±»åˆ—è¡¨
- `date_start`: å¼€å§‹æ—¥æœŸ (æ ¼å¼: YYYYMMDDHHMM)
- `date_end`: ç»“æŸæ—¥æœŸ (æ ¼å¼: YYYYMMDDHHMM)
- `max_results`: æœ€å¤§ç»“æœæ•°
- `sort_by`: æ’åºæ–¹å¼ ("relevance", "submittedDate", "lastUpdatedDate")

#### ä¾¿æ·çš„å­¦ç§‘åˆ†ç±»æœç´¢æ–¹æ³•

##### `search_ai_papers(max_results: int = 10) -> List[Paper]`

æœç´¢äººå·¥æ™ºèƒ½ç›¸å…³è®ºæ–‡ã€‚

```python
async with ArxivClient() as client:
    papers = await client.search_ai_papers(max_results=10)
```

##### `search_ml_papers(max_results: int = 10) -> List[Paper]`

æœç´¢æœºå™¨å­¦ä¹ ç›¸å…³è®ºæ–‡ã€‚

```python
async with ArxivClient() as client:
    papers = await client.search_ml_papers(max_results=10)
```

##### `search_cv_papers(max_results: int = 10) -> List[Paper]`

æœç´¢è®¡ç®—æœºè§†è§‰ç›¸å…³è®ºæ–‡ã€‚

```python
async with ArxivClient() as client:
    papers = await client.search_cv_papers(max_results=10)
```

##### `search_nlp_papers(max_results: int = 10) -> List[Paper]`

æœç´¢è‡ªç„¶è¯­è¨€å¤„ç†ç›¸å…³è®ºæ–‡ã€‚

```python
async with ArxivClient() as client:
    papers = await client.search_nlp_papers(max_results=10)
```

##### `search_robotics_papers(max_results: int = 10) -> List[Paper]`

æœç´¢æœºå™¨äººå­¦ç›¸å…³è®ºæ–‡ã€‚

```python
async with ArxivClient() as client:
    papers = await client.search_robotics_papers(max_results=10)
```

#### å…¶ä»–å®ç”¨æ–¹æ³•

##### `search_multiple_queries(search_queries: List[SearchQuery]) -> List[Paper]`

å¹¶è¡Œæœç´¢å¤šä¸ªæŸ¥è¯¢å¹¶åˆå¹¶ç»“æœã€‚

```python
async with ArxivClient() as client:
    queries = [
        SearchQuery(original_query="machine learning", max_results=5),
        SearchQuery(original_query="deep learning", max_results=5)
    ]
    
    papers = await client.search_multiple_queries(queries)
    print(f"æ€»å…±æ‰¾åˆ° {len(papers)} ç¯‡è®ºæ–‡")
```

##### `search_by_category_keyword(keyword: str, max_results: int = 10) -> List[Paper]`

æ ¹æ®å…³é”®è¯æœç´¢ç›¸å…³å­¦ç§‘åˆ†ç±»çš„è®ºæ–‡ã€‚

```python
async with ArxivClient() as client:
    papers = await client.search_by_category_keyword("machine learning", max_results=10)
```

## æ•°æ®æ¨¡å‹

### Paper ç±»

è¡¨ç¤º arXiv è®ºæ–‡çš„æ•°æ®æ¨¡å‹ã€‚

```python
from datetime import datetime
from typing import List, Optional, Dict
from pydantic import BaseModel

class Paper(BaseModel):
    title: str                              # è®ºæ–‡æ ‡é¢˜
    authors: List[str]                      # ä½œè€…åˆ—è¡¨
    abstract: str                           # è®ºæ–‡æ‘˜è¦
    arxiv_id: str                          # arXiv ID
    published: datetime                     # å‘è¡¨æ—¥æœŸ
    updated: Optional[datetime] = None      # æ›´æ–°æ—¥æœŸ
    categories: List[str] = []             # å­¦ç§‘åˆ†ç±»
    pdf_url: str                           # PDFé“¾æ¥
    entry_id: str                          # å®Œæ•´æ¡ç›®ID
    summary: Optional[str] = None           # è®ºæ–‡æ€»ç»“
    links: List[Dict[str, str]] = []       # ç›¸å…³é“¾æ¥
```

**å±æ€§è¯´æ˜:**

- `title`: è®ºæ–‡æ ‡é¢˜
- `authors`: ä½œè€…å§“ååˆ—è¡¨
- `abstract`: è®ºæ–‡æ‘˜è¦æˆ–ç®€ä»‹
- `arxiv_id`: arXiv å”¯ä¸€æ ‡è¯†ç¬¦ (å¦‚ "2023.12345")
- `published`: è®ºæ–‡é¦–æ¬¡æäº¤æ—¥æœŸ
- `updated`: è®ºæ–‡æœ€åæ›´æ–°æ—¥æœŸ (å¦‚æœæœ‰)
- `categories`: å­¦ç§‘åˆ†ç±»åˆ—è¡¨ (å¦‚ ["cs.AI", "cs.LG"])
- `pdf_url`: PDF ä¸‹è½½é“¾æ¥
- `entry_id`: arXiv æ¡ç›®å®Œæ•´ID
- `summary`: è®ºæ–‡æ€»ç»“ (å¯é€‰)
- `links`: ç›¸å…³é“¾æ¥åˆ—è¡¨

### SearchQuery ç±»

æœç´¢æŸ¥è¯¢é…ç½®æ¨¡å‹ã€‚

```python
class SearchQuery(BaseModel):
    original_query: str                     # åŸå§‹æŸ¥è¯¢
    english_queries: List[str] = []         # è‹±æ–‡æŸ¥è¯¢åˆ—è¡¨
    search_terms: List[str] = []           # æœç´¢å…³é”®è¯
    field_queries: List[SearchFieldQuery] = []  # å­—æ®µæŸ¥è¯¢
    submitted_date_start: Optional[str] = None   # å¼€å§‹æ—¥æœŸ
    submitted_date_end: Optional[str] = None     # ç»“æŸæ—¥æœŸ
    max_results: int = 10                   # æœ€å¤§ç»“æœæ•°
    sort_by: str = "relevance"             # æ’åºæ–¹å¼
    sort_order: str = "descending"         # æ’åºé¡ºåº
    recommendation_priority: str = "relevance"  # æ¨èä¼˜å…ˆçº§
```

### SearchFieldQuery ç±»

ç‰¹å®šå­—æ®µæœç´¢æŸ¥è¯¢æ¨¡å‹ã€‚

```python
class SearchFieldQuery(BaseModel):
    field: Literal["ti", "au", "abs", "co", "jr", "cat", "rn", "all"] = "all"
    terms: List[str]                        # æœç´¢å…³é”®è¯
```

**æ”¯æŒçš„å­—æ®µ:**

- `ti`: æ ‡é¢˜ (Title)
- `au`: ä½œè€… (Author)
- `abs`: æ‘˜è¦ (Abstract)
- `co`: è¯„è®º (Comment)
- `jr`: æœŸåˆŠå¼•ç”¨ (Journal Reference)
- `cat`: å­¦ç§‘åˆ†ç±» (Category) - å¿…é¡»ä½¿ç”¨ arXiv å®˜æ–¹åˆ†ç±»ä»£ç 
- `rn`: æŠ¥å‘Šå· (Report Number)
- `all`: ä¸»è¦å†…å®¹å­—æ®µ (Title + Abstract + Author + Comment)

### SearchResult ç±»

æœç´¢ç»“æœå°è£…æ¨¡å‹ã€‚

```python
class SearchResult(BaseModel):
    query: SearchQuery                     # æ‰§è¡Œçš„æŸ¥è¯¢
    papers: List[Paper]                   # æœç´¢ç»“æœ
    total_found: int                      # æ€»æ‰¾åˆ°æ•°é‡
    search_time: float                    # æœç´¢è€—æ—¶
```

## æ”¯æŒçš„ arXiv åˆ†ç±»

### è®¡ç®—æœºç§‘å­¦ (cs.*)

- `cs.AI` - äººå·¥æ™ºèƒ½
- `cs.LG` - æœºå™¨å­¦ä¹ 
- `cs.CV` - è®¡ç®—æœºè§†è§‰ä¸æ¨¡å¼è¯†åˆ«
- `cs.CL` - è®¡ç®—ä¸è¯­è¨€
- `cs.RO` - æœºå™¨äººå­¦
- `cs.IR` - ä¿¡æ¯æ£€ç´¢
- `cs.DB` - æ•°æ®åº“
- `cs.DS` - æ•°æ®ç»“æ„ä¸ç®—æ³•
- `cs.SE` - è½¯ä»¶å·¥ç¨‹
- `cs.NI` - ç½‘ç»œä¸äº’è”ç½‘æ¶æ„
- `cs.CR` - å¯†ç å­¦ä¸å®‰å…¨
- `cs.HC` - äººæœºäº¤äº’
- `cs.DC` - åˆ†å¸ƒå¼ã€å¹¶è¡Œå’Œé›†ç¾¤è®¡ç®—
- `cs.GT` - è®¡ç®—æœºç§‘å­¦ä¸åšå¼ˆè®º
- `cs.GR` - å›¾å½¢å­¦

### æ•°å­¦ (math.*)

- `math.ST` - ç»Ÿè®¡ç†è®º
- `math.OC` - ä¼˜åŒ–ä¸æ§åˆ¶
- `math.PR` - æ¦‚ç‡è®º
- `math.NA` - æ•°å€¼åˆ†æ
- `math.CO` - ç»„åˆæ•°å­¦
- `math.AG` - ä»£æ•°å‡ ä½•
- `math.AP` - åå¾®åˆ†æ–¹ç¨‹åˆ†æ
- `math.NT` - æ•°è®º
- `math.DG` - å¾®åˆ†å‡ ä½•
- `math.FA` - æ³›å‡½åˆ†æ

### ç‰©ç† (physics.*)

- `physics.comp-ph` - è®¡ç®—ç‰©ç†
- `physics.data-an` - æ•°æ®åˆ†æã€ç»Ÿè®¡ä¸æ¦‚ç‡
- `physics.app-ph` - åº”ç”¨ç‰©ç†
- `physics.optics` - å…‰å­¦
- `physics.bio-ph` - ç”Ÿç‰©ç‰©ç†
- `physics.chem-ph` - åŒ–å­¦ç‰©ç†
- `physics.med-ph` - åŒ»å­¦ç‰©ç†
- `physics.flu-dyn` - æµä½“åŠ¨åŠ›å­¦

### ç»Ÿè®¡å­¦ (stat.*)

- `stat.ML` - æœºå™¨å­¦ä¹ 
- `stat.ME` - ç»Ÿè®¡æ–¹æ³•
- `stat.AP` - åº”ç”¨ç»Ÿè®¡
- `stat.CO` - è®¡ç®—ç»Ÿè®¡
- `stat.TH` - ç»Ÿè®¡ç†è®º

### ç”µæ°”å·¥ç¨‹ (eess.*)

- `eess.AS` - éŸ³é¢‘ä¸è¯­éŸ³å¤„ç†
- `eess.IV` - å›¾åƒä¸è§†é¢‘å¤„ç†
- `eess.SP` - ä¿¡å·å¤„ç†
- `eess.SY` - ç³»ç»Ÿä¸æ§åˆ¶

### å®šé‡ç”Ÿç‰©å­¦ (q-bio.*)

- `q-bio.NC` - ç¥ç»ä¸è®¤çŸ¥
- `q-bio.BM` - ç”Ÿç‰©å¤§åˆ†å­
- `q-bio.GN` - åŸºå› ç»„å­¦
- `q-bio.QM` - å®šé‡æ–¹æ³•

### å®šé‡é‡‘è (q-fin.*)

- `q-fin.CP` - è®¡ç®—é‡‘è
- `q-fin.MF` - æ•°å­¦é‡‘è
- `q-fin.RM` - é£é™©ç®¡ç†
- `q-fin.ST` - ç»Ÿè®¡é‡‘è

## é”™è¯¯å¤„ç†

### å¸¸è§é”™è¯¯

### 1. æ— æ•ˆçš„åˆ†ç±»ä»£ç 

   ```python
   # é”™è¯¯ç¤ºä¾‹
   papers = await client.search_by_category(["invalid_category"])
   # ä¼šæŠ›å‡º: ValueError: Invalid arXiv categories: ['invalid_category']
   ```

### 2. ç½‘ç»œè¶…æ—¶

   ```python
   # è®¾ç½®æ›´é•¿çš„è¶…æ—¶æ—¶é—´
   client = ArxivClient(timeout=60)
   ```

### 3. æŸ¥è¯¢ç»“æœä¸ºç©º

   ```python
   result = await client.search_papers(query)
   if not result.papers:
       print("æœªæ‰¾åˆ°ç›¸å…³è®ºæ–‡")
   ```

### æœ€ä½³å®è·µ

### 1. ä½¿ç”¨å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨

   ```python
   async with ArxivClient() as client:
       # æœç´¢æ“ä½œ
       pass
   ```

### 2. åˆç†è®¾ç½®ç»“æœæ•°é‡

   ```python
   # é¿å…è¯·æ±‚è¿‡å¤šç»“æœ
   papers = await client.search_ml_papers(max_results=50)  # åˆç†èŒƒå›´
   ```

### 3. å¼‚å¸¸å¤„ç†

   ```python
   try:
       papers = await client.search_by_category(["cs.AI"])
   except ValueError as e:
       print(f"åˆ†ç±»é”™è¯¯: {e}")
   except Exception as e:
       print(f"æœç´¢å¤±è´¥: {e}")
   ```

## æ€§èƒ½ä¼˜åŒ–

### å¹¶è¡Œæœç´¢

```python
# å¹¶è¡Œæœç´¢å¤šä¸ªæŸ¥è¯¢
queries = [
    SearchQuery(original_query="machine learning", max_results=10),
    SearchQuery(original_query="deep learning", max_results=10),
    SearchQuery(original_query="neural network", max_results=10)
]

papers = await client.search_multiple_queries(queries)
```

### ç¼“å­˜ç»“æœ

```python
# ç¼“å­˜çƒ­é—¨æŸ¥è¯¢ç»“æœ
cache = {}

async def cached_search(query_str):
    if query_str in cache:
        return cache[query_str]
    
    async with ArxivClient() as client:
        papers = await client.search_ml_papers(max_results=10)
        cache[query_str] = papers
        return papers
```

## ç¤ºä¾‹å’Œç”¨ä¾‹

### 1. æ–‡çŒ®ç»¼è¿°åŠ©æ‰‹

```python
async def literature_review(topic: str, max_papers: int = 50):
    """æ–‡çŒ®ç»¼è¿°åŠ©æ‰‹"""
    async with ArxivClient() as client:
        # æœç´¢ç›¸å…³è®ºæ–‡
        papers = await client.search_by_category_keyword(topic, max_papers)
        
        # æŒ‰å¹´ä»½åˆ†ç»„
        by_year = {}
        for paper in papers:
            year = paper.published.year
            if year not in by_year:
                by_year[year] = []
            by_year[year].append(paper)
        
        # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
        print(f"æ‰¾åˆ° {len(papers)} ç¯‡å…³äº '{topic}' çš„è®ºæ–‡")
        for year in sorted(by_year.keys(), reverse=True):
            print(f"{year}: {len(by_year[year])} ç¯‡")
        
        return papers
```

### 2. ä½œè€…ç ”ç©¶è¿½è¸ª

```python
async def track_author_research(author_name: str):
    """è¿½è¸ªä½œè€…ç ”ç©¶"""
    async with ArxivClient() as client:
        papers = await client.search_by_author([author_name], max_results=100)
        
        # æŒ‰åˆ†ç±»ç»Ÿè®¡
        categories = {}
        for paper in papers:
            for cat in paper.categories:
                categories[cat] = categories.get(cat, 0) + 1
        
        print(f"{author_name} çš„ç ”ç©¶é¢†åŸŸåˆ†å¸ƒ:")
        for cat, count in sorted(categories.items(), key=lambda x: x[1], reverse=True):
            print(f"  {cat}: {count} ç¯‡")
        
        return papers
```

### 3. è¶‹åŠ¿åˆ†æ

```python
async def analyze_trends(keywords: List[str], years: int = 5):
    """åˆ†æç ”ç©¶è¶‹åŠ¿"""
    from datetime import datetime, timedelta
    
    async with ArxivClient() as client:
        end_date = datetime.now()
        start_date = end_date - timedelta(days=365 * years)
        
        results = {}
        for keyword in keywords:
            papers = await client.advanced_search(
                abstract_terms=[keyword],
                date_start=start_date.strftime("%Y%m%d0000"),
                date_end=end_date.strftime("%Y%m%d2359"),
                max_results=1000
            )
            
            # æŒ‰å¹´ä»½ç»Ÿè®¡
            yearly_count = {}
            for paper in papers:
                year = paper.published.year
                yearly_count[year] = yearly_count.get(year, 0) + 1
            
            results[keyword] = yearly_count
        
        return results
```

## æ›´æ–°æ—¥å¿—

### 1.0.0 (2025-07-06)

- åˆå§‹ç‰ˆæœ¬å‘å¸ƒ
- åŸºæœ¬æœç´¢åŠŸèƒ½
- å¤šå­—æ®µæœç´¢æ”¯æŒ
- å¼‚æ­¥ API
- å‘½ä»¤è¡Œå·¥å…·
- å®Œæ•´çš„æ•°æ®æ¨¡å‹
- æ”¯æŒæ‰€æœ‰ arXiv åˆ†ç±»

## è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ MIT è®¸å¯è¯ã€‚è¯¦æƒ…è¯·è§ [LICENSE](LICENSE) æ–‡ä»¶ã€‚

## è´¡çŒ®

æ¬¢è¿è´¡çŒ®ä»£ç ï¼è¯·æŸ¥çœ‹ [CONTRIBUTING.md](CONTRIBUTING.md) äº†è§£è¯¦æƒ…ã€‚

## è”ç³»æ–¹å¼

å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·é€šè¿‡ä»¥ä¸‹æ–¹å¼è”ç³»ï¼š

- GitHub Issues: [é¡¹ç›®åœ°å€](https://github.com/your-username/arxiv-search-sdk)
- é‚®ç®±: `your-email@example.com`

---

Â© 2025 ArXiv Search SDK. All rights reserved.
